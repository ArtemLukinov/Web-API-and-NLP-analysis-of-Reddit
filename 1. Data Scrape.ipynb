{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "clear-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "charged-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-tourist",
   "metadata": {},
   "source": [
    "Function that will go through the subreddit's pages and deliver 100 posts at a time 10 times (length of list), using our parameters, using the last item to determine the time it was created and setting that time as the start of the new scrape. Then combining all 10 dataframes into one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "static-violation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_subs(sub):\n",
    "    params = {\n",
    "        'subreddit': sub,\n",
    "        'size': 100,\n",
    "    }\n",
    "    df = []\n",
    "    while len(df) < 20:  # this is where the length of the data can be adjusted\n",
    "        posts = requests.get(url, params).json()\n",
    "        final_df = pd.DataFrame(posts['data'])\n",
    "        params['before'] = final_df['created_utc'].min()\n",
    "        df.append(final_df)\n",
    "\n",
    "    return pd.concat(df, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-harrison",
   "metadata": {},
   "source": [
    "Running the function for 'startrek' subreddit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "polyphonic-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "pull_subs('startrek').to_csv('./data/startrek2000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-dover",
   "metadata": {},
   "source": [
    "Running the function for 'StarWars' subreddit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "funded-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "pull_subs('StarWars').to_csv('./data/StarWars2000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-smooth",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
